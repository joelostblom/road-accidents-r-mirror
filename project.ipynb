{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing traffic mortality in the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a code cell without any tag. You can put convenience code here,\n",
    "# but it won't be included in any way in the final project.\n",
    "# For example, to be able to run tests locally in the notebook\n",
    "# you need to install the following:\n",
    "# install.packages(\"devtools\")\n",
    "# install.packages(testthat\")\n",
    "# devtools::install_github('datacamp/IRkernel.testthat')\n",
    "\n",
    "# This allows .... to be used as placeholder value in the sample code cells\n",
    ".... <- NULL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 1. Identifying the raw data ﬁles and determining their format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "![](img/car_accident.jpg)\n",
    "\n",
    "While the rate of fatal road accidents rate have been decreasing steadily since the 80's, the past 10 years have seen a stagnation in this reduction. Coupled with the increase number of miles driven in the nation, the total number of traﬃc related fatalities has now reached a 10 year high and is rapidly increasing.\n",
    "\n",
    "![](img/accident-history.png)\n",
    "\n",
    "Per request of the US Department of Transportation we are currently investigating how to derive a strategy to reduce the incidence of road accidents across the nation. By looking at the demographics of traﬃc accident victims for each US state, we find that there is a lot of variation between states. Now we want to understand if there are patterns in this variation in order to derive suggestions for a policy action plan. In particular, instead of implementing a costly nation-wide plan we want to focus on groups of  states with similar profiles. How can we find such groups in a statistically sound way and communicate the result effectively?  \n",
    "\n",
    "To accomplish these tasks, we will make use of data wrangling, plotting, dimensionality reduction, and unsupervised clustering.\n",
    "\n",
    "The data given to us was originally collected by the National Highway Traﬃc Safety Administration and the National Association of Insurance Commissioners. This particular dataset was compiled and released as a [CSV-ﬁle](https://github.com/ﬁvethirtyeight/data/tree/master/bad-drivers) by FiveThirytEight under the [CC-BY4.0 license](https://github.com/ﬁvethirtyeight/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Explore your current folder and have a look at the main dataset file.\n",
    "\n",
    "- Check the name of the current folder using `getwd()` .\n",
    "- List all files in this folder using `list.files()`.\n",
    "- View the first 20 lines of road-accidents.csv in the datasets folder using `readLines()` and setting the `n` argument.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Good to know\n",
    "\n",
    "This Project lets you practice the skills from [Introduction to the Tidyverse](https://www.datacamp.com/courses/introduction-to-the-tidyverse), which  includes the pipe operator, summarizing data, and visualizing with ggplot2; from [Correlation and Regression](https://www.datacamp.com/courses/correlation-and-regression), which includes computing correlations and regression coefficients; and from [Unsupervised Learning in R](https://www.datacamp.com/courses/unsupervised-learning-in-r), which includes PCA and KMeans clustering. We recommend that you take these courses before starting this Project.\n",
    "\n",
    "Helpful links:\n",
    "- For this task it is useful to know how to [work with files and folders in R](https://www.masterdataanalysis.com/r/working-with-files-and-folders-in-r/) \n",
    "- tidyverse [cheat sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Tidyverse+Cheat+Sheet.pdf)\n",
    "- ggplot [cheat sheet](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the name of the current folder\n",
    "current_dir <- .... \n",
    "print(current_dir)\n",
    "\n",
    "# List all files in this folder\n",
    "file_list <- .... \n",
    "print(file_list)\n",
    "\n",
    "# List files inside the datasets folder\n",
    "file_list_ds <- .... \n",
    "print(file_list_ds)\n",
    "\n",
    "# View the first 20 lines of road-accidents.csv in the datasets folder\n",
    "accidents_head <- .... \n",
    "print(accidents_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the name of the current folder\n",
    "current_dir <- getwd() \n",
    "print(current_dir)\n",
    "\n",
    "# List all files in this folder\n",
    "file_list <- list.files() \n",
    "print(file_list)\n",
    "\n",
    "# List files inside the datasets folder\n",
    "file_list_ds <- list.files(path = 'datasets') \n",
    "print(file_list_ds)\n",
    "\n",
    "\n",
    "# View the first 20 lines of road-accidents.csv in the datasets folder\n",
    "accidents_head <- readLines('datasets/road-accidents.csv', n=20) \n",
    "print(accidents_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "# These packages need to be loaded in the first @tests cell. \n",
    "library(testthat) \n",
    "library(IRkernel.testthat)\n",
    "\n",
    "# Then follows one or more tests of the student's code. \n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    expect_true(current_dir == getwd(), \n",
    "        info = \"Did you correctly assign the current_dir variable?\")\n",
    "    expect_true( 'road-accidents.csv'%in%file_list_ds, \n",
    "        info = \"Did you use the list.files function with the path argument to get the files inside the datasets folder?\")\n",
    "    expect_equal(file_list , list.files() , \n",
    "        info = \"Did you correctly assign the current_dir variable?\")\n",
    "    expect_true(length(accidents_head) == 20, \n",
    "        info = \"Make sure you read in the correct number of lines by using the n argument.\")\n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 2. Reading in and getting an overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "Orient outselves to get to know the data that we are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Import the main data file and start exploring the data.\n",
    "\n",
    "- Start by loading the tidyverse library.\n",
    "- Read in road-accidents.csv using `read_delim()`. \n",
    "- Save the number of rows columns with the `dim()` function.\n",
    "- Generate an overview of the data frame with the useful `str()` function. \n",
    "- Display the last six rows of the data frame using `tail()`. \n",
    "\n",
    "<hr>\n",
    "Read the documentation of `read_delim()` on how to use the `comment` and `delim` parameters\n",
    "Is the output of `tail()` what you would expect? A quick data sanity check like this can save us major headaches down the line.\n",
    "\n",
    "Helpful links:\n",
    "- For this task it is useful to know the [readr cheat sheet](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the tidyverse library\n",
    "# .... YOUR CODE FOR TASK 2 ....\n",
    "\n",
    "# Read in road-accidents.csv\n",
    "car_acc <- ....\n",
    "\n",
    "# Save the number of rows columns\n",
    "rows_and_cols <- ....\n",
    "print(rows_and_cols)\n",
    "\n",
    "# Generate an overview of the data frame\n",
    "car_acc_structure <- ....\n",
    "print(car_acc_structure)\n",
    "\n",
    "# Display the last six rows of the data frame. \n",
    "# .... YOUR CODE FOR TASK 2 ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the tidyverse library\n",
    "library(tidyverse)\n",
    "\n",
    "# Read in road-accidents.csv\n",
    "car_acc <- read_delim(file = 'datasets/road-accidents.csv', comment = '#', delim = '|')\n",
    "nrow(car_acc)\n",
    "ncol(car_acc)\n",
    "# Save the number of rows columns\n",
    "rows_and_cols <- dim(car_acc)\n",
    "print(rows_and_cols)\n",
    "\n",
    "# Generate an overview of the data frame\n",
    "car_acc_structure <- str(car_acc)\n",
    "print(car_acc_structure)\n",
    "\n",
    "# Display the last six rows of the data frame. \n",
    "tail(car_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# One or more tests of the student's code. \n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    expect_is(car_acc, \"tbl_df\" , \n",
    "        info = \"Did you use the read_delim function to read the file?\")\n",
    "    expect_equal(nrow(car_acc), 51 , \n",
    "        info = \"Did you tell R that in the file comments are indicated by # ?\")\n",
    "    expect_equal(ncol(car_acc), 5 , \n",
    "        info = \"Did you tell R that in the file a new column is indicated through | ?\")\n",
    "    expect_equal( length(rows_and_cols), 2 , \n",
    "        info = \"Did you use dim() to determine both the number of rows and columns?\")\n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 3. Creating a textual and a graphical summary of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "We now have an idea of what the dataset looks like. To further familiarize ourselves with this data, we will calculate summary statistics and produce a graphical overview of the data. The graphical overview is good to get a sense for the distribution of variables within the data, and could consist of one histogram per column. It is often a good idea to also explore the pairwise relationsship between all columns in the data set by using a using pairwise scatterplots (sometimes referred to as a \"scatterplot matrix\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Create a textual and graphical overview of data\n",
    "- Use `summary()` to create summary statistics of each dataframe column.\n",
    "- Use the pipe operator `%>%` on the car_acc dataframe to first deselect the state column and then use `plot()` to create a pairwise scatterplot. \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute summary statistics of all columns in the car_acc dataframe\n",
    "dat_summ <- ....\n",
    "print(dat_summ)\n",
    "\n",
    "# Use the pipe operator to deselect the state column and create a pairwise scatterplot of the remaining columns\n",
    "# .... YOUR CODE FOR TASK 3 ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute summary statistics of all columns in the car_acc dataframe\n",
    "dat_summ <- summary(car_acc)\n",
    "print(dat_summ)\n",
    "(dat_summ)\n",
    "# Use the pipe operator to deselect the state column and create a pairwise scatterplot of the remaining columns\n",
    "car_acc %>% \n",
    "    select(-state) %>%\n",
    "    plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "# One or more tests of the student's code. \n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    expect_equal(ncol(dat_summ), 5 , \n",
    "        info = \"Did you use the summary function to obtain a summary of car_acc?\")\n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 4. Quantifying association of features and fatal accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "We can already see some potentially interesting relationships between the target variable (the number of fatal accidents) and the feature variables (the remaining three columns).\n",
    "\n",
    "To quantify the pairwise relationships that we observed in the scatterplots, we can compute the Pearson correlation coefficient matrix. The Pearson correlation coeffcient is one of the most common methods to quantify correlation between variables and by convention the following thresholds are usually used:\n",
    "\n",
    "- 0.2 = weak\n",
    "- 0.5 = medium\n",
    "- 0.8 = strong\n",
    "- 0.9 = very strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Explore the correlation between all column-pairs in the data frame\n",
    "- Use the pipe operator to first remove the state column and then compute the correlation coefficient for all column-pairs using `cor()`. By default, the Pearson correlation coefficient will be computed\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Use the pipe operator and first remove the state column and then compute the correlation coefficient for all column-pairs \n",
    "corr_col <- ....\n",
    "corr_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Use the pipe operator and first remove the state column and then compute the correlation coefficient for all column-pairs \n",
    "corr_col <- car_acc %>% select(-state) %>% cor()\n",
    "corr_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "# One or more tests of the student's code. \n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    expect_is(corr_col, \"matrix\" , \n",
    "        info = \"Did you remove the 'state' column before computing the correlation?\")\n",
    "    expect_equal(prod(dim(corr_col)), 16 , \n",
    "        info = \"Did you use the cor function on the dataframe after removing the 'state' column?\")\n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 5. Fitting a multivariate linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "From the correlation table we see that the amount of fatal accidents is most strongly correlated with alcohol consumption (first row). But in addition, we also see that some of features are correlated with each other, for instance speeding and alcohol consumption are positively correlated. We therefore want to compute the association of the target with each feature while adjusting for the effect of the remaining features. This can be done using a multivariate linear regression.\n",
    "\n",
    "Both the multivariate regression and the correlation measure how strongly the features are associated with the outcome (fatal accidents). When comparing the regression coefficients with the correlation coefficients we will see that they are slightly different. The reason for this is that the multiple regression computes the association of a feature with an outcome, given the association with all other features, which is not accounted for when calculating the correlation coefficients.\n",
    "\n",
    "A particularly interesting case is when the correlation coefficient and the regression coefficient of the same feature have opposite signs. How can this be? For example, when a feature A is positively correlated with the outcome Y but also positively correlated with a different feature B that has a negative effect on Y, then the indirect correlation (A->B->Y) can overwhelm the direct correlation (A->Y). In such a case, the regression coefficient of feature A could be positive, while the correlation coefficient is negative. This is sometimes called a *masking* relationship. Let's see if the multivariate regression can reveal such a phenomenon.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Fit a multivariate linear regression model using the fatal accident rate as the outcome\n",
    "\n",
    "- Use `lm()` to fit a multivariate linear regression model. Read the documentation of the lm-function to learn how to supply the regression formula. \n",
    "- The fit-object contains various types of information. Use the `coef()` function to obtain the model coefficients.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "- the lm() function [documentation](https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Use lm to fit a multivariate linear regression model \n",
    "fit_reg <- ....\n",
    "# Retrieve the regression coefficients from the model fit\n",
    "fit_coef <- ....\n",
    "fit_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Use lm to fit a multivariate linear regression model \n",
    "fit_reg <- lm( drvr_fatl_col_bmiles ~ perc_fatl_speed + perc_fatl_alcohol + perc_fatl_1st_time , data=car_acc )\n",
    "# Retrieve the regression coefficients from the model fit\n",
    "fit_coef <- coef(fit_reg)\n",
    "fit_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "# One or more tests of the student's code.  \n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    expect_is(fit_reg, \"lm\" , \n",
    "        info = \"Did you use the lm function and are you familiar with its documentation?\")\n",
    "    expect_true( near(fit_coef[1],9.06498048340333) , \n",
    "        info = \"Did you give lm the correct linear model formula containing all three predictors?\")\n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 6. Performing PCA on standardized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "We have learned that alcohol consumption is weakly associated with the amount of fatal accidents across states. This could lead you to conclude that alcohol consumption should be a focus for futher investigations and maybe strategies should divide states into high versus low alcohol consumption in accidents. But there are also associations between  alcohol consumptions and the other two features, so it might be worth trying to split the states in a way that accounts for all three features.\n",
    "\n",
    "One way of clustering the data is to use PCA to visualize data in reduced dimensional space where we can try to pickup patterns by eye. PCA uses the absolute variance to calculate the overall variance explained for each principal component, so it is important that the features are on a similar scale (unless we would have a particular reason that one features should be weighted more).\n",
    "\n",
    "We will use the appropriate scaling function to standardize the features to be centered with mean 0 and scaled to a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Perform a Principle Component Analysis on the standardized data \n",
    "- Center and standardise the three feature columns using `scale()`\n",
    "- Perform PCA on standadized features (column 3,4,5 of car_acc_standised) using `princomp()`\n",
    "- Obtain the proportion of variance explained by each principle component. First extract the standard deviation from the PCA fit by accessing its `sdev` argument and convert it to the variance by squaring the result. Then divide the resulting vector by its sum.\n",
    "- Plot the proportion of variance explained by providin the `plot()`  function with the pve vector.\n",
    "- Compute the cumulative proportion of variance explained by applying the `cumsum()` function to the pve vector.\n",
    "- Get the cumulative proportion of variance explained by the first 2 principle components by subsetting second element of the cve. \n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "- princomp [documentation](https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/princomp)\n",
    "- PCA [lecture video](https://campus.datacamp.com/courses/unsupervised-learning-in-r/dimensionality-reduction-with-pca?ex=5) of Unsupervised Learning in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Center and standardise the three feature columns\n",
    "car_acc_standised <- car_acc %>% \n",
    "                mutate( perc_fatl_speed=....,\n",
    "                        perc_fatl_alcohol=....,\n",
    "                        perc_fatl_1st_time=.... )\n",
    "\n",
    "# Perform PCA on standadized features (column 3,4,5 of car_acc_standised)\n",
    "pca_fit <- ....\n",
    "\n",
    "# Obtain the proportion of variance explained by each principle component\n",
    "pr_var <- ....\n",
    "pve <- ....\n",
    "\n",
    "# Plot the proportion of variance explained\n",
    "....( .... , xlab=\"Principal Component\",\n",
    "      ylab=\"Proportion of Variance Explained\", type=\"b\",ylim=c(0,1))\n",
    "\n",
    "# Compute the cumulative proportion of variance explained \n",
    "cve <- ....\n",
    "# Get the cumulative proportion of variance explained by the first 2 principle components\n",
    "cve_pc2 <- ....\n",
    "print(cve_pc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Center and standade the three feature columns\n",
    "car_acc_standised <- car_acc %>% \n",
    "                mutate( perc_fatl_speed=scale(perc_fatl_speed),\n",
    "                        perc_fatl_alcohol=scale(perc_fatl_alcohol),\n",
    "                        perc_fatl_1st_time=scale(perc_fatl_1st_time) )\n",
    "\n",
    "# Perform PCA on standadized features (column 3,4,5 of car_acc_standised)\n",
    "pca_fit <- princomp( car_acc_standised[,c(3,4,5)]  )\n",
    "# Obtain the proportion of variance explained by each principle component\n",
    "pr_var <- pca_fit$sdev^2\n",
    "pve <- pr_var/sum(pr_var)\n",
    "\n",
    "# Plot the proportion of variance explained\n",
    "plot( pve , xlab=\"Principal Component\",\n",
    "      ylab=\"Proportion of Variance Explained\", type=\"b\",ylim=c(0,1))\n",
    "\n",
    "# Compute the cumulative variance explained \n",
    "cve <- cumsum(pve)\n",
    "cve[2]\n",
    "# Get the cumulative variance explained by the first 2 principle components\n",
    "cve_pc2 <- cve[2]\n",
    "print(cve_pc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "# One or more tests of the student's code. \n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    expect_true( near( mean(car_acc_standised$perc_fatl_speed)+\n",
    "                      mean(car_acc_standised$perc_fatl_alcohol)+\n",
    "                      mean(car_acc_standised$perc_fatl_1st_time),0) , \n",
    "        info = \"Did you use the scale function to create car_acc_standised?\")\n",
    "    expect_true( near(pr_var[1] , 1.34332588935974 ) , \n",
    "        info = \"Did you compute the proportion of explained variance by taking the square of pca_fit$sdev?\")\n",
    "    expect_true( near(cve[2] ,  0.794697860810483 ) , \n",
    "        info = \"Did you correctly compute the cumulative variance explained from pve by using the cumsum function?\")\n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 7. Visualizing the data using the ﬁrst two principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "The first two principal components enable visualization of the data in two dimensions while capturing a high proportion of the variation (79%) from all three features: speeding, under alcohol influence, first-time accident. This enables us to use our eyes to try to discern patterns in the data, with the goal to find groups of similar states. Although clustering algorithms are becoming increasingly efficient, human pattern recognition is an easy accessible and very efficient method of assessing patterns in data.\n",
    "\n",
    "We will create a scatter plot of the first principle components and explore how the states cluster together in this visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Plot the individual states using the first 2 principle components\n",
    "- Plot the first 2 principle components using `plot()` and the `scores` of the pca_fit object. Use scores[,1] and scores[,2] for the difference axis (the order does not matter), and set `pch` to 16. \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the principle component scores from the PCA fit\n",
    "pcomp1 <- ....\n",
    "pcomp2 <- ....\n",
    "\n",
    "# Plot the first 2 principle components in a scatterplot\n",
    "# .... YOUR CODE FOR TASK 3 ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the principle component scores from the PCA fit\n",
    "pcomp1 <- pca_fit$scores[,1]\n",
    "pcomp2 <- pca_fit$scores[,2]\n",
    "\n",
    "# Plot the first 2 principle components in a scatterplot and add axis labels\n",
    "plot(pcomp1,pcomp2,pch=16,\n",
    "     xlab=\"Principle Component 1\",\n",
    "     ylab=\"Principle Component 2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "# One or more tests of the student's code.\n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    expect_true( near(sum(pcomp1+pcomp2),-2.74780198594726e-15) , \n",
    "        info = \"Did you extract the principle compenent scores from the PCA fit using $scores?\")\n",
    "    \n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 8. Finding clusters of similar states in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "It was not entirely clear from the PCA scatter plot how many groups the states cluster in. To assist with identifying a reasonable number of clusters, we can use KMeans clustering by creating a scree plot and finding the \"elbow\" of the plot, which is an indication of when the addition of more clusters does not add much explanatory power. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Apply the K-mean method using a range of clusters and plot the within-cluster sum-of-squares.\n",
    "\n",
    "- Create a vector of k= 1 to 10 clusters.\n",
    "- Initialise vector of inertias of the same length as k_vec and filled with NAs.\n",
    "- For each k, fit a K-mean model using `kmeans()` with k clusters (centers) and save it in the mykm list\n",
    "- Obtain the within-cluster sum-of-squares from the K-means fit object using `tot.withinss`.\n",
    "- Finally, plot the within-cluster sum-of-squares against the different numbers of clusters \n",
    " \n",
    "<hr>\n",
    "Helpful links:\n",
    "- kmeans [documentation](https://www.rdocumentation.org/packages/RcmdrMisc/versions/2.5-1/topics/KMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# create a vector of 1 to 10 clusters\n",
    "k_vec <- ....\n",
    "# initialise vector of inertias\n",
    "inertias <- ....\n",
    "# initialise empty list to save K-mean fits \n",
    "mykm <- list()\n",
    "# set the seed of random number generator \n",
    "set.seed(1)\n",
    "for (k in k_vec) {\n",
    "    # for each k, fit a K-mean model with k clusters and save it in the mykm list\n",
    "                mykm[[k]] <- ....( car_acc_standised[,c(3,4,5)] , .... , nstart=50  )\n",
    "    # for each k, get the within-cluster sum-of-squares and save\n",
    "                inertias[k] <- mykm[[k]]$....             \n",
    "}\n",
    "# plot the within-cluster sum-of-squares against the number of clusters used\n",
    "plot( ....,.... , type=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# create a vector of 1 to 10 clusters\n",
    "k_vec <- 1:10\n",
    "# initialise vector of inertias, or within-cluster sum-of-squares\n",
    "inertias <- rep(NA, length(k_vec))\n",
    "# initialise empty list to save K-mean fits \n",
    "mykm <- list()\n",
    "# set the seed of random number generator \n",
    "set.seed(1)\n",
    "for (k in k_vec) {\n",
    "    # for each k, fit a K-mean model with k clusters and save it in the mykm list\n",
    "                mykm[[k]] <- kmeans( car_acc_standised[,c(3,4,5)] , centers=k , nstart=50  )\n",
    "    # for each k, get the within-cluster sum-of-squares and save\n",
    "                inertias[k] <- mykm[[k]]$tot.withinss             \n",
    "}\n",
    "# plot the within-cluster sum-of-squares against the number of clusters used\n",
    "plot( k_vec,inertias , type=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "# One or more tests of the student's code. \n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    expect_equal( sum(k_vec) , 55 , \n",
    "        info = \"Did you create a vector that goes from 1 to 10 by increments of 1?\")\n",
    "    expect_is( mykm[[1]] , 'kmeans' , \n",
    "        info = \"Did you use to kmeans function to populate mykm?\")\n",
    "    expect_equal( max(mykm[[9]]$cluster) , 9 , \n",
    "        info = \"Did you use of the 'centers' argument of the kmean function so that the number of centers changes with each iteration?\")\n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The recommended number of tasks in a DataCamp Project is between 8 and 10, so feel free to add more if necessary. You can't have more than 12 tasks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 9. Using KMeans to visualize clusters in the PCA visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "Since there wasn't a clear elbow in the scree plot, assigning the states to either 2 or 3 clusters is a reasonable choice and we will resume our analysis using 3 clusters. Let's see how the PCA scatter plot looks if we color the states according to the cluster they are assigned to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Highlight the clusters of the K-means fit with three clusters in the PCA scatterplot.\n",
    "- Obtain cluster-ids from the kmeans fit with k=3, using the `cluster` handle. \n",
    "- Then color the points of the principle component plot according to their cluster number by setting the `col` argument equal to the cluster_id vector. \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Obtain cluster-ids from the kmeans fit with k=3\n",
    "cluster_id <- as.factor(mykm[[3]]$....)\n",
    "# Color the points of the principle component plot according to their cluster number\n",
    "plot(pca_fit$scores[,1],pca_fit$scores[,2],....,pch=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Obtain cluster-ids from the kmeans fit with k=3\n",
    "cluster_id <- as.factor(mykm[[3]]$cluster)\n",
    "# Color the points of the principle component plot according to their cluster number\n",
    "plot(pca_fit$scores[,1],pca_fit$scores[,2],col=cluster_id,pch=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "# One or more tests of the student's code. \n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    expect_equal( length(unique(cluster_id)), 3 , \n",
    "        info = \"Did you access the cluster element of mykm?\")\n",
    "    \n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 10. Visualizing the feature differences between the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "Thus far, we have used both our visual interpretation of the data and the KMeans clustering algorithm to reveal patterns in the data, but what do these patterns mean?\n",
    "\n",
    "Remember that the information we have used to cluster the states into three distinct groups are the percentage of drivers speeding, under alcohol influence and that has not previously been involved in an accident. We used these clusters to visualize how the states group together when considering the first two principal components. This is good for us to understand structure in the data, but not always easy to understand, especially not if the findings are to be communicated to a non-specialist audience.\n",
    "\n",
    "A reaonsable next step in our analysis is to explore how the three clusters are different in terms of the three features that we used for clustering. Instead of using the scaled features, we return to using the unscaled features to help us interpret the differences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Visualise the distribution of speeding, alcohol influence and precentage of first time accidents in a direct comparison of the clusters \n",
    "- Add the cluster_id vector into the original dataframe\n",
    "- First remove the drvr_fatl_col_bmiles column, the use the `gather()` function to create a long format of the dataframe, which is a very useful step for many ggplot methods. \n",
    "- Visualise the distibution of the three features using `geom_violin()`, and use the `fill`-aestetic to show separate violin plots for each cluster. Depending on taste, flipping the coordinates might be nice. \n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "- dplyr's gather() function [documentation](https://www.rdocumentation.org/packages/tidyr/versions/0.8.1/topics/gather)\n",
    "- tidyr's [cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# add the cluster_id into the original dataframe\n",
    "car_acc$cluster <- ....\n",
    "# use the pipe operator on the car_acc dataframe to first remove the drvr_fatl_col_bmiles column, then convert the dataframe into a long format and then create a violin plot using ggplot\n",
    "car_acc %>%  .... %>% \n",
    "                ....(key=feature,value=percent,-state,-cluster) %>% \n",
    "                ggplot( aes(x=feature,y=percent , ....) ) +\n",
    "                .... +\n",
    "                coord_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# add the cluster_id into the original dataframe\n",
    "car_acc$cluster <- cluster_id\n",
    "\n",
    "# use the pipe operator on the car_acc dataframe to first remove the drvr_fatl_col_bmiles column, then convert the dataframe into a long format and then create a violin plot using ggplot\n",
    "car_acc %>%  select(-drvr_fatl_col_bmiles) %>% \n",
    "                gather(key=feature,value=percent,-state,-cluster) %>% \n",
    "                ggplot( aes(x=feature,y=percent , fill=cluster) ) +\n",
    "                geom_violin( ) +\n",
    "                coord_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "# One or more tests of the student's code. \n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    expect_equal( length(unique(car_acc$cluster)) , 3 , \n",
    "        info = \"Did you add the cluster_id vector to the carr_acc dataframe?\")\n",
    "    p <- last_plot()\n",
    "    expect_is(p$layers[[1]]$geom, \"GeomViolin\", info = \"Did you create a plot with geom_violin()?\")\n",
    "    \n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 11. Find out which clusters have the highest incidence of fatal accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "Now it is clear that different groups of states may require different interventions. Since resources and time are limited, it is useful to start off with an intervention in one of the three groups first. Which group would this be? To determine this, we will include data on how many miles are driven in each state, because this will help us to compute the total number of fatal accidents in each state. Data on miles driven is available in another tab-delimited text file. We will assign this new information to a column in the data frame and create a violin plot for how many total fatal traffic accidents there are within each state cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Add state-wise information about miles driven to compute total number of fatal accidents and total accidents across clusters\n",
    "- Use the `left_join` function to join the new dataframe miles_driven to car_acc. Join `by` the state variable. \n",
    "- Create a new variable num_drvr_fatl_col by multiplying drvr_fatl_col_bmiles with million_miles_annually and dividing by 1000\n",
    "- After creating a summary per cluster of the total number of fatal accidents, plot the sum for each cluster using `geom_bar` by setting the `stat` to \"identity\". You can also drop the legend using `show.legend` argument. \n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "- dplyr's left_join() function [documentation](https://dplyr.tidyverse.org/reference/join.html)\n",
    "- joining data [lecture video](https://campus.datacamp.com/courses/joining-data-in-r-with-dplyr/mutating-joins?ex=7) from Joining Data in R with dplyr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# reading in the miles-driven.csv file\n",
    "miles_driven <- read_delim( file=\"datasets/miles-driven.csv\", delim = '|' )\n",
    "# join miles_driven with car_acc\n",
    "carr_acc_joined <- ....(car_acc, miles_driven, ....) \n",
    "# create a new variable that is the total number of deadly accidents\n",
    "carr_acc_joined <- carr_acc_joined %>% mutate( num_drvr_fatl_col=.... )\n",
    "\n",
    "# use the pipe operator to groupe the new dataframe, select relevant variables and summarise \n",
    "carr_acc_joined_summ <- carr_acc_joined %>% group_by(cluster) %>% select(cluster,num_drvr_fatl_col) %>% \n",
    "                summarise(count=n(),\n",
    "                          mean=mean(num_drvr_fatl_col),\n",
    "                          sum=sum(num_drvr_fatl_col))\n",
    "print(carr_acc_joined_summ)\n",
    "\n",
    "# Compare the total fatal accident sum across clusters using a bar plot\n",
    "carr_acc_joined_summ %>% ggplot( aes(x=cluster,y=sum) ) +\n",
    "                ....( aes(fill=cluster), stat = \"identity\" , show.legend = F )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# reading in the miles-driven.csv file\n",
    "miles_driven <- read_delim( file=\"datasets/miles-driven.csv\", delim = '|' )\n",
    "# join miles_driven with car_acc\n",
    "carr_acc_joined <- left_join(car_acc, miles_driven, by=\"state\") \n",
    "\n",
    "# create a new variable that is the total number of deadly accidents\n",
    "carr_acc_joined <- carr_acc_joined %>% mutate( num_drvr_fatl_col=drvr_fatl_col_bmiles*million_miles_annually/1000 )\n",
    "\n",
    "# use the pipe operator to groupe the new dataframe, select relevant variables and summarise \n",
    "carr_acc_joined_summ <- carr_acc_joined %>% group_by(cluster) %>% select(cluster,num_drvr_fatl_col) %>% \n",
    "                summarise(count=n(),\n",
    "                          mean=mean(num_drvr_fatl_col),\n",
    "                          sum=sum(num_drvr_fatl_col))\n",
    "print(carr_acc_joined_summ)\n",
    "\n",
    "# Compare the total fatal accident sum across clusters using a bar plot\n",
    "carr_acc_joined_summ %>% ggplot( aes(x=cluster,y=sum) ) +\n",
    "                geom_bar( aes(fill=cluster), stat = \"identity\" , show.legend = F )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "# One or more tests of the student's code. \n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    expect_is(miles_driven, \"tbl_df\" , \n",
    "        info = \"Did you load the miles-driven.csv file using read_delim()?\")\n",
    "    expect_equal(sum(dim(carr_acc_joined)), 59 , \n",
    "        info = \"Did you use the left_join() function and joined the dataframes by the state column?\")\n",
    "    p <- last_plot()\n",
    "    expect_is(p$layers[[1]]$geom, \"GeomBar\", info = \"Did you create a plot with geom_bar()?\")\n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 12. Making a decision when there is no clear correct choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "As we can see, there is no obvious correct choice regarding which cluster is the most important to focus on. Yet, we can still argue for a certain cluster and motivate this using our findings above. Which cluster do you think should be a focus for policy intervention and further investigation?\n",
    "\n",
    "There are several ways to justify each cluster choice.\n",
    "- 1, Red = total number of people in accidents the largest.\n",
    "- 2, Green = The highest alcohol consumption among fatal cases, which had the strongest association with accidents.\n",
    "- 3, Blue = Cluster with the fewest number of states, good for a pilot effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Which cluster will you choose?\n",
    "\n",
    "<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a data frame named `my_data` like this after loading the tidyverse package:\n",
    "\n",
    "```r\n",
    "my_data <- read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Assign the cluster that you want recommend to start with\n",
    "cluster_num <- ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Assign the cluster that you want recommend to start with\n",
    "cluster_num <- sample(1:3,size=1)\n",
    "print(cluster_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "# One or more tests of the student's code. \n",
    "# The @solution should pass the tests.\n",
    "# The purpose of the tests is to try to catch common errors and to \n",
    "# give the student a hint on how to resolve these errors.\n",
    "run_tests({\n",
    "    test_that(\"the answer is correct\", {\n",
    "    })\n",
    "    # You can have more than one test\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
